{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate & Ingest — Manual Labeling Tool\n",
    "\n",
    "Label indexed image crops and send annotations directly to Qdrant.\n",
    "\n",
    "**Workflow:** Run cells top-to-bottom. For each crop:\n",
    "- Type the text you see → Enter\n",
    "- Type a topic tag → Enter (or Enter to skip)\n",
    "- Enter with no text = skip crop\n",
    "- Type `q` = stop annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from qdrant_client.models import SparseVector\n",
    "\n",
    "from rag.client import get_client\n",
    "from rag.config import settings\n",
    "from rag.embeddings.bge_embedder import embed_dense, embed_sparse\n",
    "from rag.schema import ensure_collection_exists\n",
    "from ocr_agent.tools import _apply_high_contrast\n",
    "\n",
    "INPUT_DIR = Path(\"../data/input\")\n",
    "\n",
    "client = get_client()\n",
    "ensure_collection_exists(client)\n",
    "print(\"Connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_unannotated(limit: int = 20) -> list:\n",
    "    \"\"\"Fetch points with empty confirmed_text from Qdrant.\"\"\"\n",
    "    results = []\n",
    "    offset = None\n",
    "    while len(results) < limit:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=settings.COLLECTION_NAME,\n",
    "            limit=min(100, limit - len(results)),\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        for p in points:\n",
    "            if not p.payload.get(\"confirmed_text\", \"\"):\n",
    "                results.append(p)\n",
    "                if len(results) >= limit:\n",
    "                    break\n",
    "        if offset is None:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_crop(source_image_id: str, region: dict) -> Image.Image | None:\n",
    "    \"\"\"Load source image and crop to region coords.\"\"\"\n",
    "    for ext in (\".jpeg\", \".jpg\", \".png\", \".tiff\", \".bmp\"):\n",
    "        path = INPUT_DIR / f\"{source_image_id}{ext}\"\n",
    "        if path.exists():\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            x, y, w, h = region[\"x\"], region[\"y\"], region[\"w\"], region[\"h\"]\n",
    "            return img.crop((x, y, x + w, y + h))\n",
    "    # Try base64 from payload as fallback\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_crop_from_b64(b64: str) -> Image.Image:\n",
    "    \"\"\"Decode a base64 image crop.\"\"\"\n",
    "    return Image.open(io.BytesIO(base64.b64decode(b64))).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def display_crop(crop: Image.Image, idx: int, total: int):\n",
    "    \"\"\"Show original and enhanced crop side by side.\"\"\"\n",
    "    enhanced = _apply_high_contrast(crop)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    ax1.imshow(crop)\n",
    "    ax1.set_title(f\"Original  [{idx+1}/{total}]\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(enhanced, cmap=\"gray\")\n",
    "    ax2.set_title(f\"Enhanced  [{idx+1}/{total}]\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def update_annotation(point_id: str, text: str, topic_tag: str, chunk_type: str):\n",
    "    \"\"\"Update a Qdrant point with text embeddings and annotation metadata.\"\"\"\n",
    "    dense_vecs = embed_dense([text])\n",
    "    sparse_vecs = embed_sparse([text])\n",
    "\n",
    "    client.set_payload(\n",
    "        collection_name=settings.COLLECTION_NAME,\n",
    "        payload={\n",
    "            \"confirmed_text\": text,\n",
    "            \"raw_ocr_text\": text,\n",
    "            \"confidence_score\": 1.0,\n",
    "            \"from_human_review\": True,\n",
    "            \"chunk_type\": chunk_type,\n",
    "            \"topic_tags\": [topic_tag] if topic_tag else [],\n",
    "            \"annotated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        },\n",
    "        points=[point_id],\n",
    "    )\n",
    "\n",
    "    client.update_vectors(\n",
    "        collection_name=settings.COLLECTION_NAME,\n",
    "        points=[\n",
    "            {\n",
    "                \"id\": point_id,\n",
    "                \"vector\": {\n",
    "                    \"text_dense\": dense_vecs[0],\n",
    "                    \"text_sparse\": sparse_vecs[0],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return point_id\n",
    "\n",
    "\n",
    "print(f\"Helpers loaded. Images dir: {INPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch unannotated crops and start annotation loop\n",
    "crops = fetch_unannotated(limit=30)\n",
    "print(f\"Found {len(crops)} unannotated crops.\\n\")\n",
    "\n",
    "annotated_count = 0\n",
    "\n",
    "for idx, point in enumerate(crops):\n",
    "    payload = point.payload\n",
    "    source_id = payload.get(\"source_image_id\", \"?\")\n",
    "    region = payload.get(\"region_coords\", {})\n",
    "\n",
    "    # Try loading crop from local file, fall back to stored base64\n",
    "    crop_img = load_crop(source_id, region)\n",
    "    if crop_img is None:\n",
    "        b64 = payload.get(\"image_crop_base64\", \"\")\n",
    "        if b64:\n",
    "            crop_img = load_crop_from_b64(b64)\n",
    "        else:\n",
    "            print(f\"  [{idx+1}/{len(crops)}] Cannot load crop for {source_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "    display_crop(crop_img, idx, len(crops))\n",
    "    print(f\"  Source: {source_id}  Region: y={region.get('y')}, h={region.get('h')}\")\n",
    "\n",
    "    text = input(f\"  [{idx+1}/{len(crops)}] Text (Enter=skip, q=stop): \").strip()\n",
    "    if text.lower() == \"q\":\n",
    "        print(\"Stopping.\")\n",
    "        break\n",
    "    if not text:\n",
    "        print(\"  Skipped.\")\n",
    "        continue\n",
    "\n",
    "    tag = input(\"  Topic tag (Enter=skip): \").strip()\n",
    "\n",
    "    # Auto chunk_type\n",
    "    wc = len(text.split())\n",
    "    chunk_type = \"word\" if wc <= 2 else (\"phrase\" if wc <= 6 else \"sentence\")\n",
    "\n",
    "    pid = update_annotation(point.id, text, tag, chunk_type)\n",
    "    annotated_count += 1\n",
    "    print(f\"  Annotated: {pid} ({chunk_type})\\n\")\n",
    "\n",
    "print(f\"\\nDone. Annotated {annotated_count} crops.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "info = client.get_collection(settings.COLLECTION_NAME)\n",
    "total = info.points_count\n",
    "\n",
    "annotated = 0\n",
    "tags: dict[str, int] = {}\n",
    "offset = None\n",
    "while True:\n",
    "    points, offset = client.scroll(\n",
    "        collection_name=settings.COLLECTION_NAME,\n",
    "        limit=100, offset=offset, with_payload=True, with_vectors=False,\n",
    "    )\n",
    "    for p in points:\n",
    "        if p.payload.get(\"confirmed_text\", \"\"):\n",
    "            annotated += 1\n",
    "            for t in p.payload.get(\"topic_tags\", []):\n",
    "                tags[t] = tags.get(t, 0) + 1\n",
    "    if not points or offset is None:\n",
    "        break\n",
    "\n",
    "print(f\"Collection: {settings.COLLECTION_NAME}\")\n",
    "print(f\"Total points:  {total}\")\n",
    "print(f\"  Annotated:   {annotated}\")\n",
    "print(f\"  Unannotated: {total - annotated}\")\n",
    "if tags:\n",
    "    print(\"\\nTopic tags:\")\n",
    "    for tag, count in sorted(tags.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {tag}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}