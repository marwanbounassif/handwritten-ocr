{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d840481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded\n"
     ]
    }
   ],
   "source": [
    "import json, re, base64\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import requests\n",
    "\n",
    "# ── Configuration ──────────────────────────────────────────────────\n",
    "OPENAI_BASE_URL = \"http://localhost:11434/v1\"  # swap to your provider\n",
    "OPENAI_API_KEY  = \"ollama\"                     # swap as needed\n",
    "OPENAI_MODEL    = \"qwen3:32b\"                # swap as needed\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"Call an OpenAI-compatible chat endpoint. `image` is base64-encoded.\"\"\"\n",
    "    content = []\n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    resp = requests.post(\n",
    "        f\"{OPENAI_BASE_URL}/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"},\n",
    "        json={\"model\": OPENAI_MODEL, \"messages\": [{\"role\": \"user\", \"content\": content}], \"stop\": [\"\\n\\n\", \"IN:\"],\n",
    "              \"temperature\": 0.1, \"max_tokens\": 4096},\n",
    "        timeout=300,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(\"✓ Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693ae73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tier 1 functions defined\n"
     ]
    }
   ],
   "source": [
    "# ── Tier 1: Hard Baseline Metrics ─────────────────────────────────\n",
    "\n",
    "def _levenshtein(a: str, b: str) -> int:\n",
    "    \"\"\"Standard DP Levenshtein distance.\"\"\"\n",
    "    n, m = len(a), len(b)\n",
    "    dp = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        prev, dp[0] = dp[0], i\n",
    "        for j in range(1, m + 1):\n",
    "            cur = dp[j]\n",
    "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + (0 if a[i - 1] == b[j - 1] else 1))\n",
    "            prev = cur\n",
    "    return dp[m]\n",
    "\n",
    "\n",
    "def _normalize(text: str, lower: bool = False) -> str:\n",
    "    t = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return t.lower() if lower else t\n",
    "\n",
    "\n",
    "def tier1_metrics(ground_truth: str, ocr_output: str, lower: bool = False) -> dict:\n",
    "    \"\"\"CER, WER, exact-match between ground truth and OCR output.\"\"\"\n",
    "    gt = _normalize(ground_truth, lower)\n",
    "    ocr = _normalize(ocr_output, lower)\n",
    "\n",
    "    cer = _levenshtein(gt, ocr) / max(len(gt), 1)\n",
    "    gt_words, ocr_words = gt.split(), ocr.split()\n",
    "    wer = _levenshtein(\" \".join(gt_words), \" \".join(ocr_words)) / max(len(\" \".join(gt_words)), 1)\n",
    "    # Word-level WER (token-based edit distance)\n",
    "    wer_tok = _levenshtein_words(gt_words, ocr_words) / max(len(gt_words), 1)\n",
    "\n",
    "    return {\n",
    "        \"input\": ocr_output,\n",
    "        \"cer\": round(cer, 4),\n",
    "        \"wer\": round(wer, 4),\n",
    "        \"wer_token\": round(wer_tok, 4),\n",
    "        \"exact_match\": gt == ocr,\n",
    "        \"gt_chars\": len(gt),\n",
    "        \"ocr_chars\": len(ocr),\n",
    "    }\n",
    "\n",
    "\n",
    "def _levenshtein_words(a: list, b: list) -> int:\n",
    "    \"\"\"Levenshtein on word-token lists.\"\"\"\n",
    "    n, m = len(a), len(b)\n",
    "    dp = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        prev, dp[0] = dp[0], i\n",
    "        for j in range(1, m + 1):\n",
    "            cur = dp[j]\n",
    "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + (0 if a[i - 1] == b[j - 1] else 1))\n",
    "            prev = cur\n",
    "    return dp[m]\n",
    "\n",
    "print(\"✓ Tier 1 functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ad7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tier 2+3 functions defined\n"
     ]
    }
   ],
   "source": [
    "# ── Tier 2+3: LLM Evaluation & Correction ────────────────────────\n",
    "\n",
    "LLM_EVAL_PROMPT = \"\"\"\n",
    "# OCR Post-Processing\n",
    "\n",
    "You are deciphering a corrupted text. The original meaning is hidden under OCR damage. Your job is to recover it.\n",
    "\n",
    "## Rules\n",
    "1. **The meaning already exists** — you are revealing it, not creating it. Never paraphrase or inject new ideas.\n",
    "2. **Noise characters/words are common** — OCR inserts random letters, splits words, or merges them. Single letters or fragments that make no grammatical sense (like a stray \"y\" or \"a\") are likely noise or remnants of a real word.\n",
    "3. **Corruption can be severe** — a word may be completely unrecognizable. Infer it from:\n",
    "   - Grammar: What part of speech must fit here?\n",
    "   - Context: What are the surrounding words talking about?\n",
    "   - Letter traces: Any surviving letters that hint at the original? (e.g. \"Hergy\" → \"therapy\", \"alio\" → \"aliz\")\n",
    "4. **Adjacent fragments often form one word** — look for split words that reconstruct when merged (e.g. \"inter alio ing\" → \"internalizing\").\n",
    "5. **Preserve everything that isn't damaged** — keep original punctuation intent, sentence structure, and word order.\n",
    "\n",
    "## Example\n",
    "Input: The irony is, that most a y these people will be in Hergy, inter alio ing eve y thing\n",
    "Output: The irony is, that most of these people will be in therapy, internalizing everything\n",
    "\n",
    "Notice: \"a y\" → \"of\" (noise fragments replaced by contextually correct word), \"Hergy\" → \"therapy\" (letter traces + context), \"inter alio ing\" → \"internalizing\" (fragments merged), \"eve y thing\" → \"everything\" (noise letter removed).\n",
    "\n",
    "## Input\n",
    "to live and explore. The problem lies inrerally in most cases. The irony is, that most a y these people will be in Hergy, inter alio ing eve y thing but still reaching the con el usi on that the world is in pe fec and they s one how are not a rfa wu t.  God' s creations are per fed our minds, magni c tory, and oo con cio us nees are the imperfections.  The world and society are nor created by God, so Huy are due their crisis m bur the real sinners a eus, our minds jy ras ere and adam bitH he o pple. In fact, f uch the stra vau o, I should n' t hau e defended that, ir won' t my point. My point is that un l by s you are her miro ll y il for a child\n",
    "\n",
    "## Output format\n",
    "Respond with ONLY the corrected text. No preamble, no explanations, no labels, no markdown formatting. Just the clean corrected text and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tier23_llm_eval(ocr_output: str) -> dict:\n",
    "    \"\"\"Single LLM call: evaluate + correct OCR output. Returns parsed dict.\"\"\"\n",
    "    prompt = LLM_EVAL_PROMPT.format(ocr_text=ocr_output)\n",
    "\n",
    "    raw = call_llm(prompt)\n",
    "\n",
    "    return raw\n",
    "\n",
    "print(\"✓ Tier 2+3 functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de1b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# ── Full Evaluation Pipeline ──────────────────────────────────────\n",
    "\n",
    "def evaluate_ocr(\n",
    "    ocr_output: str,\n",
    "    ground_truth: Optional[str] = None,\n",
    "    lower: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full OCR evaluation: hard metrics (if GT available) + LLM scoring/correction.\n",
    "    Returns a single structured dict ready for JSON serialization.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # ── Tier 1 ────────────────────────────────────────────────────\n",
    "    if ground_truth is not None:\n",
    "        result[\"tier1_raw_vs_gt\"] = tier1_metrics(ground_truth, ocr_output, lower)\n",
    "\n",
    "    # ── Tier 2+3 ──────────────────────────────────────────────────\n",
    "    llm_response = tier23_llm_eval(ocr_output)\n",
    "\n",
    "    # ── Corrected vs GT (did post-processing help?) ───────────────\n",
    "    if ground_truth is not None:\n",
    "        result[\"tier1_corrected_vs_gt\"] = tier1_metrics(ground_truth, llm_response, lower)\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── Run it ─────────────────────────────────────────────────────────\n",
    "# Fill these in with your data:\n",
    "ocr_output = Path(\"../data/output/IMG_4737.txt\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Set to None if no ground truth is available\n",
    "gt_path = Path(\"../data/output/IMG_4737_gt.md\")\n",
    "ground_truth = gt_path.read_text(encoding=\"utf-8\") if gt_path.exists() else None\n",
    "\n",
    "result = evaluate_ocr(ocr_output, ground_truth=ground_truth)\n",
    "print(\"✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52482d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tier1_raw_vs_gt': {'input': \"to live and explore. The problem lies inrerally in most cases. The irony is, that most a y these people will be in Hergy, inter alio ing eve y thing but still reaching the con el usi on that the world is in pe fec and they s one how are not a rfa wu t.  God' s creations are per fed our minds, magni c tory, and oo con cio us nees are the imperfections.  The world and society are nor created by God, so Huy are due their crisis m bur the real sinners a eus, our minds jy ras ere and adam bitH he o pple. In fact, f uch the stra vau o, I should n' t hau e defended that, ir won' t my point. My point is that un l by s you are her miro ll y il for a child\",\n",
       "  'cer': 0.5636,\n",
       "  'wer': 0.5636,\n",
       "  'wer_token': 0.4807,\n",
       "  'exact_match': False,\n",
       "  'gt_chars': 1494,\n",
       "  'ocr_chars': 652},\n",
       " 'tier1_corrected_vs_gt': {'input': '',\n",
       "  'cer': 1.0,\n",
       "  'wer': 1.0,\n",
       "  'wer_token': 1.0,\n",
       "  'exact_match': False,\n",
       "  'gt_chars': 1494,\n",
       "  'ocr_chars': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "\n",
    "def print_eval(r: dict):\n",
    "    if \"tier1_raw_vs_gt\" in r:\n",
    "        t = r[\"tier1_raw_vs_gt\"]\n",
    "        print(\"── Tier 1: Raw OCR vs Ground Truth ──\")\n",
    "        print(f\"  CER  {t['cer']:.2%}   WER  {t['wer']:.2%}   WER(tok) {t['wer_token']:.2%}   Exact: {t['exact_match']}\")\n",
    "\n",
    "    s = r[\"scores\"]\n",
    "    print(\"\\n── LLM Scores (original OCR) ──\")\n",
    "    for k, v in s.items():\n",
    "        print(f\"  {k}: {v}/100\")\n",
    "\n",
    "    cs = r[\"correction_stats\"]\n",
    "    print(f\"\\n── Corrections: {cs['total']} total  (avg conf {cs['avg_confidence']:.2f}) ──\")\n",
    "    for label, counts in [(\"Category\", cs[\"by_category\"]), (\"Severity\", cs[\"by_severity\"])]:\n",
    "        print(f\"  {label}: {counts}\")\n",
    "\n",
    "    if \"tier1_corrected_vs_gt\" in r:\n",
    "        t2 = r[\"tier1_corrected_vs_gt\"]\n",
    "        print(\"\\n── Tier 1: Corrected vs Ground Truth ──\")\n",
    "        print(f\"  CER  {t2['cer']:.2%}   WER  {t2['wer']:.2%}   WER(tok) {t2['wer_token']:.2%}   Exact: {t2['exact_match']}\")\n",
    "        if \"tier1_raw_vs_gt\" in r:\n",
    "            delta_cer = r[\"tier1_raw_vs_gt\"][\"cer\"] - t2[\"cer\"]\n",
    "            delta_wer = r[\"tier1_raw_vs_gt\"][\"wer\"] - t2[\"wer\"]\n",
    "            print(f\"  Δ CER {delta_cer:+.2%}   Δ WER {delta_wer:+.2%}  {'(improved)' if delta_cer > 0 else '(worse)'}\")\n",
    "\n",
    "    print(f\"\\n── Summary ──\\n  {r['summary']}\")\n",
    "    print(f\"\\n── Corrected Text (first 500 chars) ──\\n{r['corrected_text'][:500]}\")\n",
    "\n",
    "print_eval(result)\n",
    "\n",
    "# Full JSON for programmatic use\n",
    "# print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handwritten-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
