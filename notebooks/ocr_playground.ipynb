{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2f33a6",
   "metadata": {},
   "source": [
    "# Handwritten OCR Playground\n",
    "\n",
    "Interactive notebook for testing and debugging the handwritten notes OCR pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d35bed",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(\"Imports loaded successfully!\")\n",
    "print(f\"HF_TOKEN set: {'Yes' if os.getenv('HF_TOKEN') and os.getenv('HF_TOKEN') != 'your_token_here' else 'No - add your token to .env'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fea66c",
   "metadata": {},
   "source": [
    "## 3. Test with a Single Image\n",
    "\n",
    "Update the `image_path` to point to your handwritten note image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your test image\n",
    "image_path = Path.cwd().parent / \"data\" / \"input\" / \"IMG_4737.jpeg\"  # Update this path\n",
    "\n",
    "# Check if image exists\n",
    "if image_path.exists():\n",
    "    print(f\"\u2713 Image found: {image_path.name}\")\n",
    "else:\n",
    "    print(f\"\u2717 Image not found: {image_path}\")\n",
    "    print(\"\\nAvailable images in data/input:\")\n",
    "    input_dir = Path.cwd().parent / \"data\" / \"input\"\n",
    "    for f in input_dir.glob(\"*\"):\n",
    "        if f.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "            print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf46598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "if image_path.exists():\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Input Image: {image_path.name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image size: {img.size}\")\n",
    "    print(f\"Image mode: {img.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a3b432",
   "metadata": {},
   "source": [
    "## \ud83c\udd95 Try GOT-OCR2_0 - Full Page OCR Model\n",
    "\n",
    "GOT-OCR2_0 from StepFun AI is designed for full-page document OCR, which should handle your notebook page much better than TrOCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GOT-OCR-2.0-hf model (HuggingFace Transformers version)\n",
    "# This version properly supports MPS/CPU unlike the original\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "# Determine device - MPS for Apple Silicon, CUDA for NVIDIA, else CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Loading GOT-OCR-2.0-hf model...\")\n",
    "\n",
    "model_got = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"stepfun-ai/GOT-OCR-2.0-hf\", \n",
    "    torch_dtype=torch.bfloat16 if device != \"cpu\" else torch.float32,\n",
    "    device_map=device\n",
    ")\n",
    "processor_got = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n",
    "\n",
    "print(\"GOT-OCR-2.0-hf model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR on the full page image with GOT-OCR-2.0-hf\n",
    "print(f\"Processing image: {image_path}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load and process the image\n",
    "inputs = processor_got(str(image_path), return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "generate_ids = model_got.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    tokenizer=processor_got.tokenizer,\n",
    "    stop_strings=\"<|im_end|>\",\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "result_got = processor_got.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GOT-OCR-2.0 FULL PAGE RESULT:\")\n",
    "print(\"=\" * 50)\n",
    "print(result_got)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b321d",
   "metadata": {},
   "source": [
    "## \ud83d\udd2c Try olmOCR-2 - Allen AI's Document OCR\n",
    "\n",
    "olmOCR from Allen AI is another powerful document OCR model, fine-tuned from Qwen2-VL. It's designed for high-quality document understanding and text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5758ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load olmOCR-2 model\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "print(\"Loading olmOCR-2 model...\")\n",
    "\n",
    "OLMOCR_MODEL = \"allenai/olmOCR-2-7B-1025-FP8\"\n",
    "\n",
    "# Both processor and model from the same repo\n",
    "processor_olm = AutoProcessor.from_pretrained(OLMOCR_MODEL)\n",
    "model_olm = AutoModelForImageTextToText.from_pretrained(\n",
    "    OLMOCR_MODEL, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "print(f\"\u2713 olmOCR-2 (FP8) loaded on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f56eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR with olmOCR-2\n",
    "print(f\"Processing image: {image_path}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Build messages with local image\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": str(image_path)},\n",
    "            {\"type\": \"text\", \"text\": \"Extract and return all the text from this handwritten document.\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Process with the bundled processor\n",
    "inputs = processor_olm.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model_olm.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "# Decode\n",
    "result_olm = processor_olm.decode(output[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"olmOCR-2 RESULT:\")\n",
    "print(\"=\" * 50)\n",
    "print(result_olm)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6309de",
   "metadata": {},
   "source": [
    "## \ud83d\udcc4 Save OCR Results\n",
    "\n",
    "Save the transcribed text to organized .txt and .md files in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2665cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_ocr_result(text: str, source_image: Path, output_dir: Path, formats: list = [\"txt\", \"md\"]):\n",
    "    \"\"\"\n",
    "    Save OCR result to organized text and/or markdown files.\n",
    "    \n",
    "    Args:\n",
    "        text: The transcribed text\n",
    "        source_image: Path to the source image\n",
    "        output_dir: Directory to save output files\n",
    "        formats: List of formats to save (\"txt\", \"md\", or both)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of saved file paths\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create base filename from source image\n",
    "    base_name = source_image.stem\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    if \"txt\" in formats:\n",
    "        # Save as plain text\n",
    "        txt_path = output_dir / f\"{base_name}.txt\"\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        saved_files[\"txt\"] = txt_path\n",
    "        print(f\"\u2713 Saved: {txt_path}\")\n",
    "    \n",
    "    if \"md\" in formats:\n",
    "        # Save as markdown with metadata\n",
    "        md_path = output_dir / f\"{base_name}.md\"\n",
    "        md_content = f\"\"\"# OCR Transcription: {source_image.name}\n",
    "\n",
    "**Source:** `{source_image.name}`  \n",
    "**Processed:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}  \n",
    "**Model:** GOT-OCR-2.0-hf\n",
    "\n",
    "---\n",
    "\n",
    "## Transcribed Text\n",
    "\n",
    "{text}\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by Handwritten OCR Pipeline*\n",
    "\"\"\"\n",
    "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(md_content)\n",
    "        saved_files[\"md\"] = md_path\n",
    "        print(f\"\u2713 Saved: {md_path}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Define output directory\n",
    "output_dir = Path.cwd().parent / \"data\" / \"output\"\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51579ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current OCR result\n",
    "saved = save_ocr_result(\n",
    "    text=result_got,\n",
    "    source_image=image_path,\n",
    "    output_dir=output_dir,\n",
    "    formats=[\"txt\", \"md\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c84b1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handwritten-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}